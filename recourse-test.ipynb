{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for using actionable-recourse, provided on https://github.com/ustunb/actionable-recourse\n",
    "\n",
    "In order to compare recourse for several similar classifiers, we use cross validation to fit several logistic regression models (Is this the right way?). In the next step, we want to check whether the flipsets generated for one of them apply also for the other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold as CVGenerator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import recourse as rs\n",
    "from recourse.builder import ActionSet #FIX\n",
    "from recourse.flipset import Flipset #FIX\n",
    "from recourse.auditor import RecourseAuditor #FIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ustunb/actionable-recourse/master/examples/paper/data/credit_processed.csv'\n",
    "df = pd.read_csv(url, skipinitialspace=True)\n",
    "y, X = df.iloc[:, 0], df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Use Cross validation to train several different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_C = False\n",
    "n_splits = 20\n",
    "if not alter_C:\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    cv = cross_validate(clf, X, y, cv=n_splits, return_estimator=True)\n",
    "    cv_scores = cv['test_score']\n",
    "    classifiers = np.array(cv['estimator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: Use GridSearchCV on parameter C\n",
    "**TODO:** NameError: name 'tqdm_notebook' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alter_C:\n",
    "    cv_generator = CVGenerator(n_splits = 10, random_state = 42)\n",
    "\n",
    "    # this code is for general purpose train/test evaluation using GridSearchCV\n",
    "    gridsearch = GridSearchCV(\n",
    "        clf, param_grid={\"C\":[1.0 / np.exp(l) for l in np.linspace(0, 3, num=n_splits)]},\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score=True,\n",
    "        cv=cv_generator,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    gridsearch.fit(X,y)\n",
    "    grid_search_df = pd.DataFrame(gridsearch.cv_results_)\n",
    "\n",
    "    # cache a model for each parameter combination, trained on all data\n",
    "    model_dict = {}\n",
    "    classifiers = []\n",
    "    grid_search_df['key'] = pd.np.nan\n",
    "    for idx, p in tqdm_notebook(list(grid_search_df.params.iteritems())):\n",
    "        model = copy(clf.set_params(**p)).fit(X,y)\n",
    "\n",
    "        key = '__'.join(map(lambda x: '%s_%s' % x, p.items()))\n",
    "        model_dict[key] = model\n",
    "        grid_search_df.loc[idx, 'key'] = key\n",
    "        classifiers.append(model)\n",
    "# MAYDO: To actually use this variant, the next part would have to be adapted\n",
    "    grid_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Select those classifiers that achieve performance within certain tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826\n",
      "0.009025580929287106\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#X_test = X[:3]\n",
    "#for i, est in enumerate(scores['estimator']):\n",
    "#    print(scores['test_score'][i], est.predict(X_test))\n",
    "tolerance = 2*np.std(cv_scores)\n",
    "good_classifiers = classifiers[cv_scores >= np.max(cv_scores) - tolerance]\n",
    "\n",
    "print(np.max(cv_scores))\n",
    "print(cv_scores.std())\n",
    "print(len(good_classifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = [clf.predict(X) for clf in good_classifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customize the set of actions and align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sets=[]\n",
    "for clf in good_classifiers:\n",
    "    ## matrix of features. ActionSet will learn default bounds and step-size.\n",
    "    A = ActionSet(X)\n",
    "    ## specify immutable variables\n",
    "    A['Married'].mutable = False \n",
    "    ## can only specify properties for multiple variables using a list\n",
    "    A[['Age_lt_25', 'Age_in_25_to_40', 'Age_in_40_to_59', 'Age_geq_60']].mutable = False \n",
    "    A['EducationLevel'].step_direction = 1  ## force conditional immutability.\n",
    "    A['EducationLevel'].step_size = 1  ## set step-size to a custom value.\n",
    "    A['EducationLevel'].step_type = \"absolute\"  ## force conditional immutability.\n",
    "    A['EducationLevel'].bounds = (0, 3)\n",
    "    A['TotalMonthsOverdue'].step_size = 1  ## set step-size to a custom value.\n",
    "    A['TotalMonthsOverdue'].step_type = \"absolute\"  ## discretize on absolute values of feature rather than percentile values\n",
    "    A['TotalMonthsOverdue'].bounds = (0, 100)  ## set bounds to a custom value.\n",
    "    \n",
    "    ## tells `ActionSet` which directions each feature should move in to produce positive change.\n",
    "    A.align(clf)\n",
    "    action_sets.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessary: testing if action sets were aligned correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,clf in enumerate(good_classifiers):\n",
    "    for i,c in enumerate(X.columns):\n",
    "        if action_sets[j]._elements[c].flip_direction != np.sign(clf.coef_[:,i]):\n",
    "            print(\"Not well aligned\", j, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessary: build a flipset for one individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "#discard the output of this cell (fs.populate prints way to much)\n",
    "j_clf = 0 # choose one classifier\n",
    "i = np.flatnonzero(yhat[j_clf] <= 0)[0] # first individuum with negative outcome\n",
    "x = copy.deepcopy(X.iloc[i]).to_numpy()\n",
    "x_old = copy.deepcopy(X.iloc[i]).to_numpy()\n",
    "fs = Flipset(x = x, action_set = action_sets[j_clf], clf = good_classifiers[j_clf])\n",
    "fs.populate(enumeration_type = 'distinct_subsets', total_items = 10)\n",
    "\n",
    "print(X.iloc[i])\n",
    "print(x)\n",
    "k = 0\n",
    "# apply the k-th action in the flipset\n",
    "for j, f in enumerate(fs._df['features'][k]):\n",
    "    fi = fs._df['feature_idx'][j]\n",
    "    print(\"set\", f, \"from\", x[fi], \"to\", fs._df['x_new'][k][j])\n",
    "    x[fi] = fs._df['x_new'][k][j]\n",
    "print(x)\n",
    "\n",
    "# check, for which classifiers the outcome has changed\n",
    "for clf in good_classifiers:\n",
    "    print(clf.predict(x_old.reshape(1,-1)), \"to\", clf.predict(x.reshape(1,-1)))\n",
    "\n",
    "# display flipset\n",
    "from IPython.display import HTML\n",
    "HTML(fs.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: change inputs according to flipsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "j_clf = 0 # flipset generated for j_th classifier TODO later: iterate j_clf?\n",
    "k_fs = 0  # k-th flipset is applied; MAYDO later: iterate k_fs?\n",
    "        # when iterating j_clf, we would probably not filter X here...\n",
    "xs = copy.deepcopy(X.iloc[np.flatnonzero(yhat[j_clf] <= 0)]).to_numpy()\n",
    "for i in range(len(xs)):\n",
    "    fs = Flipset(x = xs[i], action_set = action_sets[j_clf], clf = good_classifiers[j_clf])\n",
    "    fs.populate(enumeration_type = 'distinct_subsets', total_items = 10)\n",
    "    for j, fi in enumerate(fs._df['feature_idx'][k_fs]):\n",
    "        xs[i,fi] = fs._df['x_new'][k][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Measure for how many individuals the adjusted input leads to desirable outcomes for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.5926910299003322,\n",
       " 0.6853820598006645,\n",
       " 0.48372093023255813,\n",
       " 0.4601328903654485,\n",
       " 0.6863787375415282,\n",
       " 0.5395348837209303,\n",
       " 0.6790697674418604,\n",
       " 0.5877076411960133]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flips = []\n",
    "for clf in good_classifiers:\n",
    "    ys = clf.predict(xs)\n",
    "    flips.append(np.mean(ys))\n",
    "flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Adapt from here to end: (How) do we want t use the auditor?\n",
    "\n",
    "Run Recourse Audit on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1104a7a6c23e46cab93e10d11c61dc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2971.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "auditor = RecourseAuditor(action_sets[j_clf], coefficients = good_classifiers[j_clf].coef_[0], intercept = good_classifiers[j_clf].intercept_[0])\n",
    "audit_df = auditor.audit(X)  ## matrix of features over which we will perform the audit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print mean feasibility and cost of recourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.044684389267020806\n"
     ]
    }
   ],
   "source": [
    "print(audit_df['feasible'].mean())\n",
    "print(audit_df['cost'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Generate flipsets for each good classifier and test whether it changes the outcame when the ather good classifiers are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
