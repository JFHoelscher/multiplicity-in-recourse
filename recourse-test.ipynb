{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for using actionable-recourse, provided on https://github.com/ustunb/actionable-recourse\n",
    "\n",
    "In order to compare recourse for several similar classifiers, we use cross validation to fit several logistic regression models (Is this the right way?). In the next step, we want to check whether the flipsets generated for one of them apply also for the other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold as CVGenerator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import recourse as rs\n",
    "from recourse.builder import ActionSet #FIX\n",
    "from recourse.flipset import Flipset #FIX\n",
    "from recourse.auditor import RecourseAuditor #FIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ustunb/actionable-recourse/master/examples/paper/data/credit_processed.csv'\n",
    "df = pd.read_csv(url, skipinitialspace=True)\n",
    "y, X = df.iloc[:, 0], df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Use Cross validation to train several different classifiers (takes some time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_C = True\n",
    "n_splits = 20\n",
    "if not alter_C:\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    cv = cross_validate(clf, X, y, cv=n_splits, return_estimator=True)\n",
    "    cv_scores = cv['test_score']\n",
    "    classifiers = np.array(cv['estimator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: Use GridSearchCV on parameter C (takes some time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valerie/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  8.0min finished\n"
     ]
    }
   ],
   "source": [
    "if alter_C:\n",
    "    cv_generator = CVGenerator(n_splits = 10)\n",
    "\n",
    "    # this code is for general purpose train/test evaluation using GridSearchCV\n",
    "    gridsearch = GridSearchCV(\n",
    "        clf, param_grid={\"C\":[1.0 / np.exp(l) for l in np.linspace(0, 3, num=n_splits)]},\n",
    "        scoring='accuracy',\n",
    "        cv=cv_generator,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    gridsearch.fit(X,y)\n",
    "    grid_search_df = pd.DataFrame(gridsearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valerie/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739a8916fe8d4a92874111f1fd684261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if alter_C:\n",
    "    # cache a model for each parameter combination, trained on all data\n",
    "    classifiers = []\n",
    "    classifier_Cs = []\n",
    "    for idx, p in tqdm(list(grid_search_df.params.iteritems())):\n",
    "        model = copy.deepcopy(clf.set_params(**p)).fit(X,y)\n",
    "        classifiers.append(model)\n",
    "        classifier_Cs.append(p.items())\n",
    "    cv_scores = grid_search_df['mean_test_score']\n",
    "    classifiers = np.array(classifiers)\n",
    "    # Is it inconsistent to consider score before fitting the whole train set? \n",
    "    # But otherwise, it would violate train-test splitting\n",
    "    # CV + fit is a bit of an overkill here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Select those classifiers that achieve performance within certain tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8058666666666667\n",
      "0.0003363025649689151\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#X_test = X[:3]\n",
    "#for i, est in enumerate(scores['estimator']):\n",
    "#    print(scores['test_score'][i], est.predict(X_test))\n",
    "tolerance = 1*np.std(cv_scores)\n",
    "good_classifiers = classifiers[cv_scores >= np.max(cv_scores) - tolerance]\n",
    "\n",
    "print(np.max(cv_scores))\n",
    "print(cv_scores.std())\n",
    "print(len(good_classifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = [clf.predict(X) for clf in good_classifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customize the set of actions and align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sets=[]\n",
    "for clf in good_classifiers:\n",
    "    ## matrix of features. ActionSet will learn default bounds and step-size.\n",
    "    A = ActionSet(X)\n",
    "    ## specify immutable variables\n",
    "    A['Married'].mutable = False \n",
    "    ## can only specify properties for multiple variables using a list\n",
    "    A[['Age_lt_25', 'Age_in_25_to_40', 'Age_in_40_to_59', 'Age_geq_60']].mutable = False \n",
    "    A['EducationLevel'].step_direction = 1  ## force conditional immutability.\n",
    "    A['EducationLevel'].step_size = 1  ## set step-size to a custom value.\n",
    "    A['EducationLevel'].step_type = \"absolute\"  ## force conditional immutability.\n",
    "    A['EducationLevel'].bounds = (0, 3)\n",
    "    A['TotalMonthsOverdue'].step_size = 1  ## set step-size to a custom value.\n",
    "    A['TotalMonthsOverdue'].step_type = \"absolute\"  ## discretize on absolute values of feature rather than percentile values\n",
    "    A['TotalMonthsOverdue'].bounds = (0, 100)  ## set bounds to a custom value.\n",
    "    \n",
    "    ## tells `ActionSet` which directions each feature should move in to produce positive change.\n",
    "    A.align(clf)\n",
    "    action_sets.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: change inputs according to flipsets (takes some time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "j_clf = 0 # flipset generated for j_th classifier TODO later: iterate j_clf?\n",
    "k_fs = 0  # k-th flipset is applied; MAYDO later: iterate k_fs?\n",
    "        # when iterating j_clf, we would probably not filter X here...\n",
    "xs = copy.deepcopy(X.iloc[np.flatnonzero(yhat[j_clf] <= 0)]).to_numpy()\n",
    "for i in range(len(xs)):\n",
    "    fs = Flipset(x = xs[i], action_set = action_sets[j_clf], clf = good_classifiers[j_clf])\n",
    "    fs.populate(enumeration_type = 'distinct_subsets', total_items = 10)\n",
    "    for j, fi in enumerate(fs._df['feature_idx'][k_fs]):\n",
    "        xs[i,fi] = fs._df['x_new'][k][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Measure for how many individuals the adjusted input leads to desirable outcomes for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.777345017851347,\n",
       " 0.8617332035053554,\n",
       " 0.8182408308990587,\n",
       " 0.8234339500162285,\n",
       " 0.801363193768257,\n",
       " 0.5894190197987667]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flips = []\n",
    "for clf in good_classifiers:\n",
    "    ys = clf.predict(xs)\n",
    "    flips.append(np.mean(ys))\n",
    "flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Recourse Audit for each classifier on Training Data (Takes some time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8015e8ebded426e862de171fddbde9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3042.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f68dbea46f4d8c8d3f44b9605300ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3020.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a6c0ee009347fcae9941fcf12352ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2945.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1815ca49d6d749879be287b17eba3834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2939.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5430a173cddd4b599ff8d02fa1970b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2967.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675c3d8dca2b43999b932f93402b27de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2965.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d97afeef9cb4521a3d1b0211e6e7403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3075.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost': [0.04641990086296696,\n",
       "  0.045324120817820154,\n",
       "  0.04475382944622765,\n",
       "  0.044295240369545893,\n",
       "  0.044933442279032,\n",
       "  0.044667496904708635,\n",
       "  0.046365966464075786],\n",
       " 'feasible': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit = {\"cost\": [], \"feasible\": []}\n",
    "for j, clf in enumerate(good_classifiers):\n",
    "    auditor = RecourseAuditor(action_sets[j], coefficients = clf.coef_[0], intercept = clf.intercept_[0])\n",
    "    audit_df = auditor.audit(X)  ## matrix of features over which we will perform the audit.\n",
    "    audit[\"feasible\"].append(audit_df['feasible'].mean())\n",
    "    audit[\"cost\"].append(audit_df['cost'].mean())\n",
    "audit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
