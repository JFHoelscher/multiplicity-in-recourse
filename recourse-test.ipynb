{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for using actionable-recourse, provided on https://github.com/ustunb/actionable-recourse\n",
    "\n",
    "In order to compare recourse for several similar classifiers, we use cross validation to fit several logistic regression models (Is this the right way?). In the next step, we want to check whether the flipsets generated for one of them apply also for the other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold as CVGenerator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import recourse as rs\n",
    "from recourse.builder import ActionSet #FIX\n",
    "from recourse.flipset import Flipset #FIX\n",
    "from recourse.auditor import RecourseAuditor #FIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ustunb/actionable-recourse/master/examples/paper/data/credit_processed.csv'\n",
    "df = pd.read_csv(url, skipinitialspace=True)\n",
    "y, X = df.iloc[:, 0], df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Use Cross validation to train several different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_C = False\n",
    "n_splits = 20\n",
    "if not alter_C:\n",
    "    clf = LogisticRegression(max_iter=10000)\n",
    "    cv = cross_validate(clf, X, y, cv=n_splits, return_estimator=True)\n",
    "    cv_scores = cv['test_score']\n",
    "    classifiers = np.array(cv['estimator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: Use GridSearchCV on parameter C\n",
    "**TODO:** NameError: name 'tqdm_notebook' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alter_C:\n",
    "    cv_generator = CVGenerator(n_splits = 10, random_state = 42)\n",
    "\n",
    "    # this code is for general purpose train/test evaluation using GridSearchCV\n",
    "    gridsearch = GridSearchCV(\n",
    "        clf, param_grid={\"C\":[1.0 / np.exp(l) for l in np.linspace(0, 3, num=n_splits)]},\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score=True,\n",
    "        cv=cv_generator,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    gridsearch.fit(X,y)\n",
    "    grid_search_df = pd.DataFrame(gridsearch.cv_results_)\n",
    "\n",
    "    # cache a model for each parameter combination, trained on all data\n",
    "    model_dict = {}\n",
    "    classifiers = []\n",
    "    grid_search_df['key'] = pd.np.nan\n",
    "    for idx, p in tqdm_notebook(list(grid_search_df.params.iteritems())):\n",
    "        model = copy(clf.set_params(**p)).fit(X,y)\n",
    "\n",
    "        key = '__'.join(map(lambda x: '%s_%s' % x, p.items()))\n",
    "        model_dict[key] = model\n",
    "        grid_search_df.loc[idx, 'key'] = key\n",
    "        classifiers.append(model)\n",
    "# MAYDO: To actually use this variant, the next part would have to be adapted\n",
    "    grid_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Select those classifiers that achieve performance within certain tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826\n",
      "0.009025580929287106\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#X_test = X[:3]\n",
    "#for i, est in enumerate(scores['estimator']):\n",
    "#    print(scores['test_score'][i], est.predict(X_test))\n",
    "tolerance = 2*np.std(cv_scores)\n",
    "good_classifiers = classifiers[cv_scores >= np.max(cv_scores) - tolerance]\n",
    "\n",
    "print(np.max(cv_scores))\n",
    "print(cv_scores.std())\n",
    "print(len(good_classifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = [clf.predict(X) for clf in good_classifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customize the set of actions and align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sets=[]\n",
    "for clf in good_classifiers:\n",
    "    ## matrix of features. ActionSet will learn default bounds and step-size.\n",
    "    A = ActionSet(X)\n",
    "    ## specify immutable variables\n",
    "    A['Married'].mutable = False \n",
    "    ## can only specify properties for multiple variables using a list\n",
    "    A[['Age_lt_25', 'Age_in_25_to_40', 'Age_in_40_to_59', 'Age_geq_60']].mutable = False \n",
    "    A['EducationLevel'].step_direction = 1  ## force conditional immutability.\n",
    "    A['EducationLevel'].step_size = 1  ## set step-size to a custom value.\n",
    "    A['EducationLevel'].step_type = \"absolute\"  ## force conditional immutability.\n",
    "    A['EducationLevel'].bounds = (0, 3)\n",
    "    A['TotalMonthsOverdue'].step_size = 1  ## set step-size to a custom value.\n",
    "    A['TotalMonthsOverdue'].step_type = \"absolute\"  ## discretize on absolute values of feature rather than percentile values\n",
    "    A['TotalMonthsOverdue'].bounds = (0, 100)  ## set bounds to a custom value.\n",
    "    \n",
    "    ## tells `ActionSet` which directions each feature should move in to produce positive change.\n",
    "    A.align(clf)\n",
    "    action_sets.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: change inputs according to flipsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "j_clf = 0 # flipset generated for j_th classifier TODO later: iterate j_clf?\n",
    "k_fs = 0  # k-th flipset is applied; MAYDO later: iterate k_fs?\n",
    "        # when iterating j_clf, we would probably not filter X here...\n",
    "xs = copy.deepcopy(X.iloc[np.flatnonzero(yhat[j_clf] <= 0)]).to_numpy()\n",
    "for i in range(len(xs)):\n",
    "    fs = Flipset(x = xs[i], action_set = action_sets[j_clf], clf = good_classifiers[j_clf])\n",
    "    fs.populate(enumeration_type = 'distinct_subsets', total_items = 10)\n",
    "    for j, fi in enumerate(fs._df['feature_idx'][k_fs]):\n",
    "        xs[i,fi] = fs._df['x_new'][k][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW: Measure for how many individuals the adjusted input leads to desirable outcomes for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.5926910299003322,\n",
       " 0.6853820598006645,\n",
       " 0.48372093023255813,\n",
       " 0.4601328903654485,\n",
       " 0.6863787375415282,\n",
       " 0.5395348837209303,\n",
       " 0.6790697674418604,\n",
       " 0.5877076411960133]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flips = []\n",
    "for clf in good_classifiers:\n",
    "    ys = clf.predict(xs)\n",
    "    flips.append(np.mean(ys))\n",
    "flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Recourse Audit for each classifier on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62478cc48f6c4c22854844474d81c95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2971.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0683b20369450cac12c937bb83fad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2932.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f74f9d28f446dc96d6b30aa0199176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3012.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0947443a87a4c9dade45afbfe6afbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3189.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662e361156ba40a5bde0d4f81b7f40f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b03525ec84e47faa3907787adcf644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2985.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604d64b64b1341e1b943fc2a5ad0e177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1b9d1ae1e14885961aea68c1b1133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2935.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2da3ab9ec54ec69a39fedef39254a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2926.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost': [0.044684389267020806,\n",
       "  0.04584822627027638,\n",
       "  0.0454822368685171,\n",
       "  0.05078450717884565,\n",
       "  0.05103499556203111,\n",
       "  0.044942292745506374,\n",
       "  0.04562744430573237,\n",
       "  0.04459665782022164,\n",
       "  0.04445740221061869],\n",
       " 'feasible': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit = {\"cost\": [], \"feasible\": []}\n",
    "for j, clf in enumerate(good_classifiers):\n",
    "    auditor = RecourseAuditor(action_sets[j], coefficients = clf.coef_[0], intercept = clf.intercept_[0])\n",
    "    audit_df = auditor.audit(X)  ## matrix of features over which we will perform the audit.\n",
    "    audit[\"feasible\"].append(audit_df['feasible'].mean())\n",
    "    audit[\"cost\"].append(audit_df['cost'].mean())\n",
    "audit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
